
References:
http://www.geeksforgeeks.org/fundamentals-of-algorithms/#AnalysisofAlgorithms

=================================================================================================================================
1) O(1): Time complexity of a function (or set of statements) is considered as O(1) if it doesnâ€™t contain loop, recursion and call to any other non-constant time function.

   // set of non-recursive and non-loop statements
For example swap() function has O(1) time complexity.
A loop or recursion that runs a constant number of times is also considered as O(1). For example the following loop is O(1).

   // Here c is a constant   
   for (int i = 1; i <= c; i++) {  
        // some O(1) expressions
   }

2) O(n): Time Complexity of a loop is considered as O(n) if the loop variables is incremented / decremented by a constant amount. For example following functions have O(n) time complexity.

   // Here c is a positive integer constant   
   for (int i = 1; i <= n; i += c) {  
        // some O(1) expressions
   }

   for (int i = n; i > 0; i -= c) {
        // some O(1) expressions
   }

3) O(npowc): Time complexity of nested loops is equal to the number of times the innermost statement is executed. For example the following sample loops have O(n2) time complexity

  
   for (int i = 1; i <=n; i += c) {
       for (int j = 1; j <=n; j += c) {
          // some O(1) expressions
       }
   }

   for (int i = n; i > 0; i += c) {
       for (int j = i+1; j <=n; j += c) {
          // some O(1) expressions
   }
For example Selection sort and Insertion Sort have O(n2) time complexity.

4) O(Logn) Time Complexity of a loop is considered as O(Logn) if the loop variables is divided / multiplied by a constant amount.

   for (int i = 1; i <=n; i *= c) {
       // some O(1) expressions
   }
   for (int i = n; i > 0; i /= c) {
       // some O(1) expressions
   }
For example Binary Search(refer iterative implementation) has O(Logn) time complexity.

5) O(LogLogn) Time Complexity of a loop is considered as O(LogLogn) if the loop variables is reduced / increased exponentially by a constant amount.

   // Here c is a constant greater than 1   
   for (int i = 2; i <=n; i = pow(i, c)) { 
       // some O(1) expressions
   }
   //Here fun is sqrt or cuberoot or any other constant root
   for (int i = n; i > 0; i = fun(i)) { 
       // some O(1) expressions
   }
See this for more explanation.
How to combine time complexities of consecutive loops?
When there are consecutive loops, we calculate time complexity as sum of time complexities of individual loops.

   for (int i = 1; i <=m; i += c) {  
        // some O(1) expressions
   }
   for (int i = 1; i <=n; i += c) {
        // some O(1) expressions
   }
   Time complexity of above code is O(m) + O(n) which is O(m+n)
   If m == n, the time complexity becomes O(2n) which is O(n).  
   
6) How to combine time complexities of consecutive loops?
When there are consecutive loops, we calculate time complexity as sum of time complexities of individual loops.

   for (int i = 1; i <=m; i += c) {  
        // some O(1) expressions
   }
   for (int i = 1; i <=n; i += c) {
        // some O(1) expressions
   }
   Time complexity of above code is O(m) + O(n) which is O(m+n)
   If m == n, the time complexity becomes O(2n) which is O(n).  
   
 7) How to calculate time complexity when there are many if, else statements inside loops?
As discussed here, worst case time complexity is the most useful among best, average and worst. Therefore we need to consider worst case. We evaluate the situation when values in if-else conditions cause maximum number of statements to be executed.
For example consider the linear search function where we consider the case when element is present at the end or not present at all.
When the code is too complex to consider all if-else cases, we can get an upper bound by ignoring if else and other complex control statements.

8) How to calculate time complexity of recursive functions?
Time complexity of a recursive function can be written as a mathematical recurrence relation. To calculate time complexity, we must know how to solve recurrences. 
=================================================================================================================================
1.  Operation: Loop
    Input size: n
         for (i=1; i<n; i++)
   
   Bigoh(n) -> Also called linear bigoh
   
2.  Operation: 2 Loops  
    Input size: n
      for (i=1; i<n; i++) {}
      for (j=1; j<n; j++) {}
   
   Bigoh(2n) -> surpress constant -> Bigoh(n)
   
3.  Operation: nested loops
    Input size: n
      for(i=0; i<n; i++)
        for(j=0; J<n; j++)
    
    Bigoh(n2) -> Also called quadratic Bigoh
    
4.  Operation: Variations of nested loops
    Input size: n
      for (i=0; i<n; i++)
        for (j=i+1; j<n; j++)
    
    Bigoh: (n-1) + (n-2) + (n-3) + ... + 2 + 1 = n(n-1)/2
    Bigoh(n2)
    
5. Operation: Binary Search Tree
    Input size: n
        if(a[mid] == key) 
            Found
        else if(a[mid] < key)
                binary_search(a, key, mid+1, n)
        else           
                binary_search(a, key, start, mid-1)
                
    Bigoh: n/2 + n/4 + n/8 + ...1
    Bigoh(log n base 2)
    
6. Solving recurrence (usually recursion)
    Usually a recursion for input size n, will divide the problems into a subsets each of size n/b (where a >= 1, b > 1)
        T(n) = a T(n/b) + f(n)
   
    
        
    
   
   
